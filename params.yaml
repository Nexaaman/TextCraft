    
TrainingArguments:
    num_train_epochs: 1
    warmup_steps: 500
    per_device_train_batch_size: 1
    per_device_eval_batch_size: 1
    weight_decay: 0.01
    logging_steps: 10
    evaluation_strategy: steps
    eval_steps: 500
    save_steps: 1e6
    gradient_accumulation_steps: 16
    optim: "paged_adamw_32bit"
    lr_scheduler_type: "cosine"
    fp16:  False
    bf16:  False

bnb_config:
    use_4bit: True
    bnb_4bit_compute_dtype: "float16"
    bnb_4bit_quant_type: "nf4"
    use_nested_quant: False

peft_config:
    lora_r: 64
    lora_alpha: 16
    lora_dropout: 0.1